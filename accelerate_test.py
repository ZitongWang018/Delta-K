import time
import torch
import warnings
import gc
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Callable
from diffusers import (
    DiffusionPipeline,
    StableDiffusionXLPipeline,
    StableDiffusion3Pipeline,
    FluxPipeline,
    EulerDiscreteScheduler,
    FlowMatchEulerDiscreteScheduler
)
from diffusers.utils import logging

# ===================== å…¨å±€ç¯å¢ƒåˆå§‹åŒ–ï¼ˆæ¶æ„é€šç”¨ï¼Œå¿…å¼€ï¼‰=====================
# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings("ignore")
logging.set_verbosity_error()

# ã€æ¶æ„æ— å…³é€šç”¨ç¡¬ä»¶åŠ é€Ÿã€‘æ‰€æœ‰GPU/æ¨¡å‹é€šç”¨ï¼Œæ— å…¼å®¹æ€§é£é™©
# 1. TF32åŠ é€Ÿï¼šAmpere+æ¶æ„æ˜¾å¡çŸ©é˜µä¹˜æ³•æé€Ÿ30%+ï¼Œæ— ç²¾åº¦æŸå¤±
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
# 2. CUDNNåŸºå‡†ä¼˜åŒ–ï¼šå›ºå®šè¾“å…¥å°ºå¯¸ä¸‹å·ç§¯/æ³¨æ„åŠ›è®¡ç®—æé€Ÿ
torch.backends.cudnn.benchmark = True
# 3. æ˜¾å­˜ç¢ç‰‡åŒ–ä¼˜åŒ–ï¼šæœç»OOMï¼Œé€‚é…å¤§æ¨¡å‹
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] += ",max_split_size_mb:128"

# ===================== ã€ç”¨æˆ·å”¯ä¸€éœ€è¦ä¿®æ”¹çš„é…ç½®åŒºã€‘=====================
@dataclass
class BenchmarkConfig:
    # -------------------------- æ ¸å¿ƒæ¨¡å‹é…ç½® --------------------------
    # æ¨¡å‹è·¯å¾„é…ç½®ï¼šæ”¯æŒä»»æ„SDç³»åˆ—/FLUXç³»åˆ—æ¨¡å‹ï¼Œè‡ªåŠ¨è¯†åˆ«æ¶æ„
    model_paths: Dict[str, str] = field(default_factory=lambda: {
        "sdxl": "/data/yulin/hf_cache/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b",
        "sd3.5medium": "/data/yulin/hf_cache/hub/models--stabilityai--stable-diffusion-3.5-medium/snapshots/b940f670f0eda2d07fbb75229e779da1ad11eb80",
        "flux": "/data/yulin/hf_cache/hub/models--black-forest-labs--FLUX.1-dev/snapshots/3de623fc3c33e44ffbe2bad470d0f45bccf2eb21"
    })
    # ç¡¬ä»¶é…ç½®
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    dtype: torch.dtype = torch.float16

    # -------------------------- ç”Ÿæˆé€šç”¨é…ç½® --------------------------
    width: int = 1024
    height: int = 1024
    prompt: str = "a beautiful sunset over the ocean, photorealistic, 8k, high detail"
    negative_prompt: str = "blurry, low quality, ugly, distorted, pixelated"

    # -------------------------- æµ‹è¯•æ§åˆ¶é…ç½® --------------------------
    test_rounds: int = 3  # æ­£å¼æµ‹è¯•è½®æ•°ï¼Œå–å¹³å‡å€¼
    warmup_rounds: int = 2  # é¢„çƒ­è½®æ•°ï¼Œå¼€å¯compileæ—¶è‡ªåŠ¨+2
    save_images: bool = True
    save_dir: str = "./speed_test_results"

    # -------------------------- åŠ é€Ÿå¼€å…³ï¼ˆæ¶æ„é€šç”¨ï¼Œä¸€é”®å¯åœï¼‰--------------------------
    enable_xformers: bool = True  # æ³¨æ„åŠ›åŠ é€Ÿï¼Œæ‰€æœ‰æ¨¡å‹é€šç”¨ï¼Œæé€Ÿ20%-40%
    enable_torch_compile: bool = False  # å›¾ç¼–è¯‘åŠ é€Ÿï¼Œé¦–æ¬¡è¿è¡Œæœ‰ç¼–è¯‘å¼€é”€ï¼Œåç»­æé€Ÿ30%-60%
    enable_int8_quant: bool = False  # INT8é‡åŒ–ï¼Œæ˜¾å­˜é™40%ï¼Œæé€Ÿ15%-25%ï¼Œæ‰€æœ‰æ¨¡å‹é€šç”¨
    torch_compile_mode: str = "max-autotune"  # ç¼–è¯‘æ¨¡å¼ï¼šmax-autotune(æ€§èƒ½æœ€ä¼˜)/reduce-overhead(å¹³è¡¡)/default(æœ€å¿«ç¼–è¯‘)

    # -------------------------- æ­¥æ•°ä¼˜åŒ–é…ç½®ï¼ˆæ¶æ„è‡ªåŠ¨é€‚é…ï¼‰--------------------------
    # å™ªå£°é¢„æµ‹æ¨¡å‹ï¼ˆSDXLç­‰ï¼‰æ­¥æ•°é…ç½®
    noise_predict_base_steps: int = 20
    noise_predict_optim_steps: int = 10
    # æµåŒ¹é…æ¨¡å‹ï¼ˆSD3/FLUXç­‰ï¼‰æ­¥æ•°é…ç½®
    flow_match_base_steps: int = 28
    flow_match_optim_steps: int = 14
    # é€šç”¨CFGé…ç½®
    base_guidance_scale: float = 4.5

# ===================== æ¶æ„æ— å…³æ ¸å¿ƒå·¥å…·åº“ =====================
def init_dirs(save_dir: str):
    """åˆå§‹åŒ–è¾“å‡ºç›®å½•"""
    import os
    os.makedirs(save_dir, exist_ok=True)
    return save_dir

def clear_gpu_memory():
    """ã€å…³é”®ã€‘å½»åº•é‡Šæ”¾GPUæ˜¾å­˜ï¼Œæœç»è·¨ç”¨ä¾‹æ˜¾å­˜æ®‹ç•™"""
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()

@dataclass
class ModelArchInfo:
    """æ¶æ„æ— å…³çš„æ¨¡å‹ä¿¡æ¯å°è£…ï¼Œè‡ªåŠ¨è¯†åˆ«"""
    model_name: str
    pipeline_type: str
    core_module: torch.nn.Module  # æ ¸å¿ƒè®¡ç®—æ¨¡å—ï¼ˆUNet/Transformerï¼‰
    core_module_name: str
    is_flow_match: bool  # æ˜¯å¦ä¸ºæµåŒ¹é…æ¨¡å‹
    support_negative_prompt: bool
    support_guidance_scale: bool
    recommend_scheduler: Callable

def auto_detect_model_arch(pipeline, model_name: str) -> ModelArchInfo:
    """ã€æ¶æ„æ— å…³æ ¸å¿ƒã€‘è‡ªåŠ¨è¯†åˆ«æ¨¡å‹æ¶æ„ï¼Œæ— éœ€ç¡¬ç¼–ç """
    # è¯†åˆ«pipelineç±»å‹
    pipeline_type = pipeline.__class__.__name__
    
    # è‡ªåŠ¨è¯†åˆ«æ ¸å¿ƒè®¡ç®—æ¨¡å—
    core_module = None
    core_module_name = ""
    if hasattr(pipeline, "unet"):
        core_module = pipeline.unet
        core_module_name = "unet"
    elif hasattr(pipeline, "transformer"):
        core_module = pipeline.transformer
        core_module_name = "transformer"
    
    # è‡ªåŠ¨è¯†åˆ«æ˜¯å¦ä¸ºæµåŒ¹é…æ¨¡å‹
    is_flow_match = isinstance(pipeline.scheduler, FlowMatchEulerDiscreteScheduler) or "Flux" in pipeline_type or "StableDiffusion3" in pipeline_type
    
    # è‡ªåŠ¨è¯†åˆ«æ”¯æŒçš„å‚æ•°
    support_negative_prompt = hasattr(pipeline, "negative_prompt") or "StableDiffusionXLPipeline" in pipeline_type
    support_guidance_scale = hasattr(pipeline, "guidance_scale") or not ("Flux" in pipeline_type)
    
    # è‡ªåŠ¨æ¨èå…¼å®¹çš„é‡‡æ ·å™¨
    recommend_scheduler = FlowMatchEulerDiscreteScheduler if is_flow_match else EulerDiscreteScheduler
    
    return ModelArchInfo(
        model_name=model_name,
        pipeline_type=pipeline_type,
        core_module=core_module,
        core_module_name=core_module_name,
        is_flow_match=is_flow_match,
        support_negative_prompt=support_negative_prompt,
        support_guidance_scale=support_guidance_scale,
        recommend_scheduler=recommend_scheduler
    )

# ===================== æ¶æ„æ— å…³åŠ é€Ÿå®ç° =====================
def load_model_with_acceleration(
    model_name: str,
    model_path: str,
    cfg: BenchmarkConfig,
    enable_quant: bool = False
):
    """ã€æ¶æ„æ— å…³ã€‘åŠ è½½æ¨¡å‹å¹¶åº”ç”¨é€šç”¨åŠ é€Ÿï¼Œè‡ªåŠ¨é€‚é…æ‰€æœ‰æ¨¡å‹"""
    print(f"\n===== åŠ è½½æ¨¡å‹ï¼š{model_name} =====")
    clear_gpu_memory()

    # åŸºç¡€åŠ è½½å‚æ•°ï¼ˆæ¶æ„é€šç”¨ï¼‰
    load_kwargs = {
        "torch_dtype": cfg.dtype,
        "use_safetensors": True,
        "low_cpu_mem_usage": True,
        "device_map": cfg.device
    }

    # ã€æ¶æ„æ— å…³INT8é‡åŒ–ã€‘å®˜æ–¹åŸç”Ÿå®ç°ï¼Œå…¼å®¹æ‰€æœ‰æ¨¡å‹ï¼Œæ— æŠ¥é”™
    if enable_quant and cfg.enable_int8_quant:
        load_kwargs["load_in_8bit"] = True
        print("âœ… INT8é‡åŒ–å·²å¯ç”¨ï¼ˆå®˜æ–¹åŸç”Ÿæ¶æ„é€šç”¨ï¼‰")

    # éFLUXæ¨¡å‹åŠ è½½fp16 variantï¼Œè‡ªåŠ¨é€‚é…
    if "flux" not in model_name.lower():
        load_kwargs["variant"] = "fp16"

    # è‡ªåŠ¨åŠ è½½pipelineï¼Œæ— éœ€ç¡¬ç¼–ç æ¨¡å‹ç±»å‹
    try:
        pipeline = DiffusionPipeline.from_pretrained(model_path, **load_kwargs)
    except Exception as e:
        raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)[:100]}")

    # è‡ªåŠ¨è¯†åˆ«æ¨¡å‹æ¶æ„
    arch_info = auto_detect_model_arch(pipeline, model_name)
    print(f"âœ… è‡ªåŠ¨è¯†åˆ«æ¨¡å‹æ¶æ„ï¼š{arch_info.pipeline_type} | {'æµåŒ¹é…æ¨¡å‹' if arch_info.is_flow_match else 'å™ªå£°é¢„æµ‹æ¨¡å‹'}")

    # 1. xformersæ³¨æ„åŠ›åŠ é€Ÿï¼ˆæ¶æ„é€šç”¨ï¼‰
    if cfg.enable_xformers:
        try:
            pipeline.enable_xformers_memory_efficient_attention()
            print("âœ… xformersæ³¨æ„åŠ›åŠ é€Ÿå·²å¯ç”¨ï¼ˆæ¶æ„é€šç”¨ï¼‰")
        except Exception as e:
            print(f"âš ï¸  xformerså¯ç”¨å¤±è´¥ï¼Œå·²è·³è¿‡ï¼š{str(e)[:60]}")

    # 2. torch.compileå›¾ç¼–è¯‘åŠ é€Ÿï¼ˆæ¶æ„æ— å…³ï¼Œè‡ªåŠ¨è¯†åˆ«æ ¸å¿ƒæ¨¡å—ï¼‰
    if cfg.enable_torch_compile and arch_info.core_module is not None:
        try:
            setattr(
                pipeline,
                arch_info.core_module_name,
                torch.compile(
                    arch_info.core_module,
                    mode=cfg.torch_compile_mode,
                    fullgraph=True,
                    dynamic=False
                )
            )
            print(f"âœ… torch.compileå·²å¯ç”¨ï¼ˆæ ¸å¿ƒæ¨¡å—ï¼š{arch_info.core_module_name}ï¼‰")
        except Exception as e:
            print(f"âš ï¸  torch.compileå¯ç”¨å¤±è´¥ï¼Œå·²è·³è¿‡ï¼š{str(e)[:60]}")

    # 3. é€šç”¨æ˜¾å­˜ä¼˜åŒ–ï¼ˆæ¶æ„é€šç”¨ï¼Œè‡ªåŠ¨åˆ¤æ–­å…¼å®¹æ€§ï¼‰
    if hasattr(pipeline, "enable_vae_slicing"):
        pipeline.enable_vae_slicing()
    if hasattr(pipeline, "enable_vae_tiling"):
        pipeline.enable_vae_tiling()
    if hasattr(pipeline, "enable_attention_slicing"):
        pipeline.enable_attention_slicing("max")

    return pipeline, arch_info

def run_inference_test(
    pipeline,
    arch_info: ModelArchInfo,
    cfg: BenchmarkConfig,
    use_optimized_steps: bool = False
):
    """ã€æ¶æ„æ— å…³ã€‘æ¨ç†æµ‹è¯•ï¼Œè‡ªåŠ¨é€‚é…æ¨¡å‹å‚æ•°ï¼Œæœç»æŠ¥é”™"""
    # 1. è‡ªåŠ¨é…ç½®æ­¥æ•°å’Œé‡‡æ ·å™¨ï¼ˆæ¶æ„é€‚é…ï¼‰
    if arch_info.is_flow_match:
        steps = cfg.flow_match_optim_steps if use_optimized_steps else cfg.flow_match_base_steps
    else:
        steps = cfg.noise_predict_optim_steps if use_optimized_steps else cfg.noise_predict_base_steps
    
    # è‡ªåŠ¨è®¾ç½®å…¼å®¹çš„é‡‡æ ·å™¨ï¼Œæœç»ç”Ÿæˆè´¨é‡å¼‚å¸¸
    pipeline.scheduler = arch_info.recommend_scheduler.from_config(pipeline.scheduler.config)
    print(f"âœ… é‡‡æ ·å™¨å·²é…ç½®ï¼š{arch_info.recommend_scheduler.__name__} | æ¨ç†æ­¥æ•°ï¼š{steps}")

    # 2. ã€æ¶æ„æ— å…³ã€‘è‡ªåŠ¨æ„å»ºæ¨ç†å‚æ•°ï¼Œæœç»ä¸æ”¯æŒçš„å‚æ•°æŠ¥é”™
    infer_kwargs = {
        "prompt": cfg.prompt,
        "width": cfg.width,
        "height": cfg.height,
        "num_inference_steps": steps,
        "generator": torch.Generator(cfg.device).manual_seed(42),
        "num_images_per_prompt": 1
    }

    # è‡ªåŠ¨æ·»åŠ æ¨¡å‹æ”¯æŒçš„å‚æ•°
    if arch_info.support_guidance_scale:
        infer_kwargs["guidance_scale"] = cfg.base_guidance_scale
    if arch_info.support_negative_prompt:
        infer_kwargs["negative_prompt"] = cfg.negative_prompt

    # 3. é¢„çƒ­å¤„ç†ï¼šå¼€å¯compileæ—¶è‡ªåŠ¨å¢åŠ é¢„çƒ­è½®æ¬¡ï¼Œç¡®ä¿ç¼–è¯‘å®Œæˆ
    final_warmup_rounds = cfg.warmup_rounds + 2 if cfg.enable_torch_compile else cfg.warmup_rounds
    print(f"é¢„çƒ­æ¨¡å‹ï¼ˆ{final_warmup_rounds}è½®ï¼‰...")
    for i in range(final_warmup_rounds):
        _ = pipeline(**infer_kwargs)

    # 4. æ­£å¼è®¡æ—¶æµ‹è¯•ï¼ˆGPUåŒæ­¥ï¼Œç¡®ä¿è®¡æ—¶å‡†ç¡®ï¼‰
    total_time = 0.0
    save_dir = init_dirs(cfg.save_dir)
    print(f"æ­£å¼æµ‹è¯•ï¼ˆ{cfg.test_rounds}è½®ï¼‰...")

    for round_idx in range(cfg.test_rounds):
        # é‡ç½®éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
        infer_kwargs["generator"] = torch.Generator(cfg.device).manual_seed(42 + round_idx)
        
        # GPUä¸¥æ ¼åŒæ­¥è®¡æ—¶ï¼Œæœç»å¼‚æ­¥æ‰§è¡Œå¯¼è‡´çš„è®¡æ—¶ä¸å‡†
        torch.cuda.synchronize()
        start_time = time.time()
        
        result = pipeline(**infer_kwargs)
        image = result.images[0]
        
        torch.cuda.synchronize()
        round_time = time.time() - start_time
        total_time += round_time

        # ä¿å­˜å›¾ç‰‡
        if cfg.save_images:
            img_name = f"{arch_info.model_name}_steps-{steps}_quant-{cfg.enable_int8_quant}_compile-{cfg.enable_torch_compile}_round-{round_idx+1}.png"
            image.save(f"{save_dir}/{img_name}")
        
        print(f"  ç¬¬{round_idx+1}è½®è€—æ—¶ï¼š{round_time:.2f}ç§’")

    avg_time = total_time / cfg.test_rounds
    print(f"âœ… æœ¬è½®æµ‹è¯•å¹³å‡è€—æ—¶ï¼š{avg_time:.2f}ç§’")
    return avg_time

# ===================== æµ‹è¯•è°ƒåº¦ä¸»æµç¨‹ =====================
def run_full_benchmark(cfg: BenchmarkConfig):
    """å…¨é‡æµ‹è¯•è°ƒåº¦ï¼Œæ¶æ„æ— å…³ï¼Œé”™è¯¯éš”ç¦»"""
    # æµ‹è¯•çŸ©é˜µï¼šæ‰€æœ‰ç»„åˆæ¶æ„é€šç”¨ï¼Œè‡ªåŠ¨é€‚é…
    test_matrix = [
        {"name": "Baseline", "optim_steps": False, "enable_quant": False},
        {"name": "ä»…æ­¥æ•°ä¼˜åŒ–", "optim_steps": True, "enable_quant": False},
        {"name": "ä»…INT8é‡åŒ–", "optim_steps": False, "enable_quant": True},
        {"name": "æ­¥æ•°ä¼˜åŒ–+INT8é‡åŒ–", "optim_steps": True, "enable_quant": True},
    ]

    final_results = {}

    # éå†æ‰€æœ‰æ¨¡å‹
    for model_name, model_path in cfg.model_paths.items():
        final_results[model_name] = {}
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æµ‹è¯•æ¨¡å‹ï¼š{model_name}")
        print(f"{'='*60}")
        print(f"å…¨å±€åŠ é€Ÿé…ç½®ï¼šxformers={cfg.enable_xformers} | torch.compile={cfg.enable_torch_compile}")

        # éå†æ‰€æœ‰æµ‹è¯•ç»„åˆ
        for test_case in test_matrix:
            test_name = test_case["name"]
            print(f"\n--- æµ‹è¯•ç»„åˆï¼š{test_name} ---")
            
            try:
                # åŠ è½½æ¨¡å‹å¹¶åº”ç”¨åŠ é€Ÿ
                pipeline, arch_info = load_model_with_acceleration(
                    model_name=model_name,
                    model_path=model_path,
                    cfg=cfg,
                    enable_quant=test_case["enable_quant"]
                )

                # æ‰§è¡Œæ¨ç†æµ‹è¯•
                avg_time = run_inference_test(
                    pipeline=pipeline,
                    arch_info=arch_info,
                    cfg=cfg,
                    use_optimized_steps=test_case["optim_steps"]
                )

                # ä¿å­˜ç»“æœ
                final_results[model_name][test_name] = f"{avg_time:.2f}ç§’"

                # ã€å…³é”®ã€‘å½»åº•é‡Šæ”¾å½“å‰æ¨¡å‹æ˜¾å­˜ï¼Œæœç»è·¨ç”¨ä¾‹OOM
                del pipeline, arch_info
                clear_gpu_memory()

            except Exception as e:
                # é”™è¯¯éš”ç¦»ï¼šå•ä¸ªç”¨ä¾‹å¤±è´¥ä¸å½±å“æ•´ä½“æµ‹è¯•
                error_msg = str(e)[:80]
                print(f"âŒ æµ‹è¯•å¤±è´¥ï¼š{error_msg}")
                final_results[model_name][test_name] = f"å¤±è´¥: {error_msg}"
                clear_gpu_memory()

    # ===================== ç»“æœè¾“å‡º =====================
    print(f"\n{'='*60}")
    print("ğŸ“Š ã€æœ€ç»ˆæµ‹è¯•ç»“æœæ±‡æ€»ã€‘ï¼ˆå•ä½ï¼šç§’ï¼‰")
    print(f"{'='*60}")
    print(f"å…¨å±€é…ç½®ï¼šxformers={cfg.enable_xformers} | torch.compile={cfg.enable_torch_compile} | INT8é‡åŒ–={cfg.enable_int8_quant}")
    print(f"æµ‹è¯•è®¾å¤‡ï¼š{cfg.device} | PyTorchç‰ˆæœ¬ï¼š{torch.__version__}")

    for model_name, model_results in final_results.items():
        print(f"\nã€{model_name}ã€‘")
        for test_name, time_str in model_results.items():
            print(f"  {test_name}: {time_str}")

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    save_dir = init_dirs(cfg.save_dir)
    result_file = f"{save_dir}/final_benchmark_results.txt"
    with open(result_file, "w", encoding="utf-8") as f:
        f.write("æ¶æ„æ— å…³å¤šæ¨¡å‹æ¨ç†åŠ é€Ÿæµ‹è¯•ç»“æœæ±‡æ€»\n")
        f.write("="*60 + "\n")
        f.write(f"æµ‹è¯•è®¾å¤‡ï¼š{cfg.device}\n")
        f.write(f"PyTorchç‰ˆæœ¬ï¼š{torch.__version__}\n")
        f.write(f"å…¨å±€åŠ é€Ÿé…ç½®ï¼šxformers={cfg.enable_xformers} | torch.compile={cfg.enable_torch_compile}\n")
        f.write("="*60 + "\n")
        for model_name, model_results in final_results.items():
            f.write(f"\nã€{model_name}ã€‘\n")
            for test_name, time_str in model_results.items():
                f.write(f"  {test_name}: {time_str}\n")
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³ï¼š{result_file}")
    return final_results

# ===================== ä¸»å‡½æ•°å…¥å£ =====================
if __name__ == "__main__":
    # åˆå§‹åŒ–é…ç½®
    benchmark_cfg = BenchmarkConfig()
    init_dirs(benchmark_cfg.save_dir)

    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print(f"{'='*60}")
    print("æ¶æ„æ— å…³å¤šæ¨¡å‹æ¨ç†åŠ é€Ÿå¯¹æ¯”æµ‹è¯•")
    print(f"{'='*60}")
    print(f"æµ‹è¯•è®¾å¤‡ï¼š{benchmark_cfg.device}")
    print(f"PyTorchç‰ˆæœ¬ï¼š{torch.__version__}")
    print(f"æµ‹è¯•æ¨¡å‹ï¼š{list(benchmark_cfg.model_paths.keys())}")
    print(f"æµ‹è¯•è½®æ•°ï¼š{benchmark_cfg.test_rounds}")
    print(f"xformersåŠ é€Ÿï¼š{'å¼€å¯' if benchmark_cfg.enable_xformers else 'å…³é—­'}")
    print(f"torch.compileï¼š{'å¼€å¯' if benchmark_cfg.enable_torch_compile else 'å…³é—­'}")
    print(f"INT8é‡åŒ–ï¼š{'å¼€å¯' if benchmark_cfg.enable_int8_quant else 'å…³é—­'}")
    
    if benchmark_cfg.enable_torch_compile:
        print(f"âš ï¸  å·²å¼€å¯torch.compileï¼Œé¦–æ¬¡è¿è¡Œä¼šæœ‰3-10åˆ†é’Ÿçš„ç¼–è¯‘å¼€é”€ï¼Œè¯·è€å¿ƒç­‰å¾…...")

    # è¿è¡Œå…¨é‡æµ‹è¯•
    run_full_benchmark(benchmark_cfg)